{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python for Data Science and AI Final Project\n",
    "\n",
    "Product Review Classification for E-commerce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install transformers datasets\n",
    "! pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from functools import reduce\n",
    "\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading in the dataset\n",
    "ds = load_dataset(\"SzilvasiPeter/amazon-shoe-review\", split = \"train\")\n",
    "ds[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading in the model\n",
    "pipe = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA\n",
    "splits = {'train': 'data/train-00000-of-00001-b73d46e3c7526716.parquet', 'test': 'data/test-00000-of-00001-036b4d091fdd5ccd.parquet'}\n",
    "eda_df = pd.read_parquet(\"hf://datasets/SzilvasiPeter/amazon-shoe-review/\" + splits[\"train\"])\n",
    "\n",
    "# check for missing values:\n",
    "eda_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(eda_df)\n",
    "display(eda_df.describe())\n",
    "display(eda_df['labels'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rating score distribution\n",
    "\n",
    "x = eda_df['labels'].unique()\n",
    "y = eda_df['labels'].value_counts()[x]\n",
    "plt.bar(x,y, color = 'teal')\n",
    "\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Rating Distribution')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rating score vs. Average character length\n",
    "eda_df['text_length'] = eda_df['text'].apply(len)\n",
    "avg_text_length = eda_df.groupby('labels')['text_length'].mean()\n",
    "plt.plot(avg_text_length.index, avg_text_length.values, marker='o', linestyle='-', color='teal')\n",
    "\n",
    "plt.xlabel('Rating Score')\n",
    "plt.ylabel('Average Number of Characters in Text')\n",
    "plt.title('Average Character Length per Rating Score')\n",
    "plt.xticks(avg_text_length.index)\n",
    "\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing\n",
    "# Lowercasing and removing special characters\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r\"<.*?>\", \"\", text) # Remove html tags such as <br>\n",
    "    words = word_tokenize(text) # Tokenize text\n",
    "    s = PorterStemmer() # Stem words\n",
    "    text = \" \".join(s.stem(w) for w in words)\n",
    "    text = ''.join(char for char in text if (char.isalnum() or char == \" \")) # remove special characters\n",
    "    return text\n",
    "\n",
    "# Apply preprocessing to the dataset (if needed)\n",
    "dataset = ds.map(lambda x: {'text': preprocess_text(x['text'])})\n",
    "dataset[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a smaller sample of the data\n",
    "sample_ds = dataset[1000:1200]\n",
    "sample_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the model\n",
    "candidate_labels = ['positive', 'negative', 'neutral']\n",
    "results = pipe(sample_ds['text'], candidate_labels, multi_label=True)\n",
    "results[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual labels\n",
    "true_labels = sample_ds['labels']  \n",
    "\n",
    "# Mapping the labels to postive, negative, and neutral\n",
    "true_labels_mapped = ['positive' if label > 3 else 'neutral' if label == 3 else 'negative' for label in true_labels]\n",
    "\n",
    "# Taking the top predicted label\n",
    "predicted_labels = [result['labels'][0] for result in results]\n",
    "\n",
    "# Comparative dataframe for true and predicted labels\n",
    "comparison_df = pd.DataFrame({\n",
    "    'True Label': true_labels_mapped,\n",
    "    'Predicted Label': predicted_labels,\n",
    "    'Confidence Score': [result['scores'][0] for result in results]\n",
    "})\n",
    "comparison_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "accuracy = accuracy_score(true_labels_mapped, predicted_labels)\n",
    "f\"Accuracy: {accuracy * 100:.2f}%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['POSITIVE', 'NEGATIVE', 'NEUTRAL'], yticklabels=['POSITIVE', 'NEGATIVE', \"NEUTRAL\"])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model : https://huggingface.co/facebook/bart-large-mnli\n",
    "\n",
    "Dataset : https://huggingface.co/datasets/SzilvasiPeter/amazon-shoe-review\n",
    "\n",
    "About Zero Shot Learning : https://joeddav.github.io/blog/2020/05/29/ZSL.html\n",
    "\n",
    "Huggingface Zero Shot Classification: https://huggingface.co/tasks/zero-shot-classification\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
